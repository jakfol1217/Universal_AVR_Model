830630 (not finished);830633,normal STSN,batch_size=10,image_size=256,,67% GPU Memory Allocated,4:27:55 - 1.47it/s
830634,3xAC STSN,batch_size=10,image_size=256,use_reentrant=True,48% GPU Memory Allocated,5:35:12 - 1.17it/s
830637,3xAC STSN,batch_size=10,image_size=256,use_reentrant=False,48% GPU Memory Allocated,5:55:42 - 1.10it/s

830649 (ERROR: OOM),normal STSN,batch_size=4,image_size=512,,,
830650,3xAC STSN,batch_size=4,image_size=512,use_reentrant=False,75% GPU Memory Allocated,26:34:10 - 0.61it/s
830882 (ERROR: OOM),3xAC STSN,batch_size=8,image_size=512,use_reentrant=False,75% GPU Memory Allocated,

830957,2xAC (decoder/encoder) STSN,batch_size=4,image_size=512,use_reentrant=False,81% GPU Memory Allocated,26:19:44 - 0.62it/s
830962,1xAC (decoder) STSN,batch_size=4,image_size=512,use_reentrant=False,81% GPU Memory Allocated,26:48:20 - 0.61it/s

830963,1xAC (decoder) STSN,batch_size=16,image_size=256,use_reentrant=False,80.5% GPU Memory Allocated,5:35:07 - 0.73it/s
830964 (ERROR: OOM),1xAC (decoder) STSN,batch_size=32,image_size=256,use_reentrant=False,,
830965 (ERROR: OOM),3xAC STSN,batch_size=32,image_size=256,use_reentrant=False,,
TODO:
# 3 GPU test new functionality to save slots every n steps
830967,1xAC (decoder) STSN,batch_size=16,image_size=256,use_reentrant=False,80.5% GPU Memory Allocated,5:35:07 - 0.73it/s
#// Check different strategy FSDS

### Change to singularity and nvidia image (should be better adjusted to the eniviroment)
# 16-mixed bf16 (1xgpu)
831286,1xAC (decoder) STSN,batch_size=16,image_size=256,use_reentrant=False,80.5% GPU Memory Allocated,7:43:30 - 0.53it/s
# 16-true f16 (1xgpu)
831290 (RuntimeError: normal expects all elements of std >= 0.0) ,1xAC (decoder) STSN,batch_size=16,image_size=256,use_reentrant=False,,
#3x GPU (16-mixed bf16)
831230,1xAC (decoder) STSN,batch_size=16,image_size=256,use_reentrant=False,81% GPU Memory Allocated,2:33:38 - 0.53it/s
### 16-true runs when only on CPU

# TODO: Other combination


### Training - 1 day:

831320 sbatch scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images
831348 sbatch scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images model=stsn_128hd batch_size=8
831352 sbatch scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images model=stsn_40slots128hd batch_size=4
831350 sbatch scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images model=stsn_40slots_5iter batch_size=8
831351 sbatch scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images model=stsn_40slots batch_size=8


# smth wrong with h5py (after checking predictions - everything seems the same) - changed stsn to run validation more frequently and store predictions from validation set (not tested - waiting for GPU to free)

## Test runs:
831911 sbatch --time=0-00:30:00 --gpus=0 --mem=100GB scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10 +trainer/num_sanity_val_steps=0
831902 sbatch --time=0-00:30:00 --gpus=0 --mem=100GB scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images_v3 +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10 +trainer/num_sanity_val_steps=0

831903 sbatch --time=0-00:30:00 --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10
831904 sbatch --time=0-00:30:00 --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images_v3 +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10

832433 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10
832434 sbatch --time=0-00:30:00 --mem=10GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10

832436 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10
832450 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images +trainer/max_time=00:00:10:00 +trainer/limit_val_batches=10 +trainer/limit_test_batches=10

832452 sbatch --time=0-00:30:00 --mem=12GB --gpus=0 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer/max_time=00:00:10:00 trainer/limit_val_batches=10 trainer/limit_test_batches=10
832453 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test
832462 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test
# check validation every 0.5 epoch
832465 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test
# check trainer without top 2 best
832467 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50


### Valid runs:

832505 sbatch --time=1-00:00:00 --mem=10GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images lr=0.001
832506 sbatch --time=1-00:00:00 --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images img_size=128 batch_size=32
832509 sbatch --time=1-00:00:00 --mem=9GB --gpus=2 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images lr=0.00005

832510 sbatch --time=1-00:00:00 --mem=16GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images model=stsnv3
832511 sbatch --time=1-00:00:00 --mem=16GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images model=stsnv3 model.num_slots=40 batch_size=8


### Run bulk tests

./scripts/run_bulk.sh 3 "--time=0-00:05:00 --mem=6GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50
#First job id: 835729
#Job id 2: 835730
#Job id 3: 835731
#First job id: 835763
#Job id 2: 835764
#Job id 3: 835765
#First job id: 835769
#Job id 2: 835770
#Job id 3: 835771

### Test 128x128

# ~1:28:46, 0.72it/s, 80.5% GPU Memory Allocated
835722 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=64
# ~1:28:46, 0.72it/s, 80.5% GPU Memory Allocated
835723 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 model=stsnv3 img_size=128 batch_size=64
# torch.cuda.OutOfMemoryError: CUDA out of memory
835724 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=128
# torch.cuda.OutOfMemoryError: CUDA out of memory
835725 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 model=stsnv3 img_size=128 batch_size=128

# ~1:17:05, 0.41it/s, 89.28% GPU Memory Allocated
835732 sbatch --time=0-00:30:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=128 model.num_slots=10
# ~2:06:19,  1.01it/s, 60.6% GPU Memory Allocated
835742 sbatch --time=0-00:05:00 --mem=12GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=32 model.num_slots=30

# ~2:45:00, 0.77it/s, 79.8% GPU Memory Allocated
835747 sbatch --time=0-00:05:00 --mem=7GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=32 model.num_slots=40
# ~3:24:43, 0.62it/s, 98.71% GPU Memory Allocated
835748 sbatch --time=0-00:05:00 --mem=7GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=32 model.num_slots=50
# torch.cuda.OutOfMemoryError: CUDA out of memory
835749 sbatch --time=0-00:05:00 --mem=7GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=32 model.num_slots=60
# ~4:03:25,  1.05it/s, 55% GPU Memory Allocated
835750 sbatch --time=0-00:05:00 --mem=7GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=16 model.num_slots=60

# torch.cuda.OutOfMemoryError: CUDA out of memory
835755 sbatch --time=0-00:05:00 --mem=10GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=128 model.num_iterations=1
# ~1:25:05, 0.75it/s, 80.47% GPU Memory Allocated
835792 sbatch --time=0-00:05:00 --mem=6GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=64 model.num_iterations=1
# ~1:28:14, 0.72it/s, 86.79% GPU Memory Allocated
835756 sbatch --time=0-00:05:00 --mem=10GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=64 model.num_iterations=5

# ~1:05:55, 0.49it/s, 87.92% GPU Memory Allocated
835793 sbatch --time=0-00:05:00 --mem=6GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=128 model.hid_dim=32
# torch.cuda.OutOfMemoryError: CUDA out of memory
835794 sbatch --time=0-00:05:00 --mem=6GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=64 model.hid_dim=128
# ~4:15:09, 0.50it/s, 80.50% GPU Memory Allocated
835795 sbatch --time=0-00:05:00 --mem=6GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images trainer=test every_n_train_steps=50 img_size=128 batch_size=32 model.hid_dim=128

# Longer scenarios 

# 1-channel
# 20 slots 3 iteration 64 hidden dim - DEFAULT ~1:28:46
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=64 trainer.val_check_interval=null

## Slots:
### 10 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=128 trainer.val_check_interval=null model.num_slots=10
### 30 slots
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model.num_slots=30
### 40 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model.num_slots=40
### 50 slots
./scripts/run_bulk.sh 6 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model.num_slots=50
### 60 slots
./scripts/run_bulk.sh 6 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=16 trainer.val_check_interval=null model.num_slots=60

## Iteration
### Iteration 1
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=64 trainer.val_check_interval=null model.num_iterations=1
### Iteration 5
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=64 trainer.val_check_interval=null model.num_iterations=5

## Hidden dimention
### hid_dim 32
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=128 trainer.val_check_interval=null model.hid_dim=32
### hid_dim 128
./scripts/run_bulk.sh 6 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model.hid_dim=128

# 3-channel
# 20 slots 3 iteration 64 hidden dim - DEFAULT
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=64 trainer.val_check_interval=null model=stsnv3
First job id: 835796,Job id 2: 835797,Job id 3: 835798
./scripts/run_bulk_2_0.sh 3 835798 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=64 trainer.val_check_interval=null model=stsnv3
Job based on: 835798, Job id 1: 839926, Job id 2: 839927, Job id 3: 839928

## Slots:
### 10 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=128 trainer.val_check_interval=null model=stsnv3 model.num_slots=10
First job id: 835799,Job id 2: 835800,Job id 3: 835801
./scripts/run_bulk_2_0.sh 3 835801 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=128 trainer.val_check_interval=null model=stsnv3 model.num_slots=10
Job based on: 835801, Job id 1: 839929, Job id 2: 839930, Job id 3: 839931
### 30 slots
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model=stsnv3 model.num_slots=30
First job id: 835802,Job id 2: 835803,Job id 3: 835804,Job id 4: 835805
./scripts/run_bulk_2_0.sh 4 835805 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model=stsnv3 model.num_slots=30
Job based on: 835805, Job id 1: 839932, Job id 2: 839933, Job id 3: 839934, Job id 4: 839935
### 40 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model=stsnv3 model.num_slots=40
First job id: 835806,Job id 2: 835807,Job id 3: 835808,Job id 4: 835809,Job id 5: 835810
./scripts/run_bulk_2_0.sh 4 835810 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model=stsnv3 model.num_slots=40
Job based on: 835810, Job id 1: 839936, Job id 2: 839937, Job id 3: 839938, Job id 4: 839939
### 50 slots
./scripts/run_bulk.sh 6 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model=stsnv3 model.num_slots=50
First job id: 835811,Job id 2: 835812,Job id 3: 835813,Job id 4: 835814,Job id 5: 835815,Job id 6: 835816
### 60 slots
./scripts/run_bulk.sh 6 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=16 trainer.val_check_interval=null model=stsnv3 model.num_slots=60
First job id: 835817,Job id 2: 835818,Job id 3: 835819,Job id 4: 835820,Job id 5: 835821,Job id 6: 835822

## Iteration
### Iteration 1
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=64 trainer.val_check_interval=null model=stsnv3 model.num_iterations=1
### Iteration 5
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=64 trainer.val_check_interval=null model=stsnv3 model.num_iterations=5

## Hidden dimention
### hid_dim 32
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=128 trainer.val_check_interval=null model=stsnv3 model.hid_dim=32
### hid_dim 128
./scripts/run_bulk.sh 6 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_images max_epochs=100 every_n_epochs=1 every_n_train_steps=null early_stopping_patience=10 img_size=128 batch_size=32 trainer.val_check_interval=null model=stsnv3 model.hid_dim=128

# Scoring - tests
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring trainer=test every_n_train_steps=50 img_size=128 batch_size=128 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 845228 OOM
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring trainer=test every_n_train_steps=50 img_size=128 batch_size=32 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 845229 OOM
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring trainer=test every_n_train_steps=50 img_size=128 batch_size=8 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 845230 OK 36.22% GPU MEM usage
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring trainer=test every_n_train_steps=50 img_size=128 batch_size=16 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 845231; 845256; 845258 OK 68.84%/77.66%/68.85% GPU MEM usage
## VASR
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring trainer=test every_n_train_steps=50 every_n_epochs=null img_size=128 batch_size=32 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 845330 68.84% GPU MEM usage

### 60 slots - Slot model frozen
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring trainer=test every_n_epochs=null every_n_train_steps=50 img_size=128 batch_size=8 model.slot_model.num_slots=60 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
# 850830 128 OOM, 850831 64 OOM, 850832 32 OOM, 850833 16 OOM, 850834 8: 88.92% GPU MEM usage
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring trainer=test every_n_epochs=null every_n_train_steps=50 img_size=128 batch_size=16 model.slot_model.num_slots=60 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
# 850835 16 99.73% GPU MEM usage

### 20 slots - Slot model trained (aux loss ratio = 10)
## BONGARD_HOI
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train trainer=test every_n_epochs=null every_n_train_steps=50 img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 857087 - a little risky but OK 95-99% GPU MEM usage
## VASR
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_full_train trainer=test every_n_epochs=null every_n_train_steps=50 img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 857088, 88% GPU MEM usage
## BONGARD_HOI+VASR
sbatch --time=0-00:15:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_vasr_scoring_full_train trainer=test every_n_epochs=null every_n_train_steps=50 img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 857091, 90-95% GPU MEM usage (26h per epoch (only train) :o)

# Scoring
## BONGARD_HOI (SLOT=BONGARD_HOI+VASR)
### 20 slots - Slot model frozen
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring img_size=128 batch_size=16 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# ~845261~  845333 # 10GB memory looks good # Increase early stopping
./scripts/run_bulk_2_0.sh 3 845333 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring img_size=128 batch_size=16 model.slot_model.num_slots=20 early_stopping_patience=100 max_epochs=500
Job based on: 845333,Job id 1: 850857,Job id 2: 850858,Job id 3: 850859

### 20 slots - Slot model trained
## BONGARD_HOI (aux loss ratio = 10)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
First job id: 857092,Job id 2: 857093,Job id 3: 857094
## VASR (aux loss ratio = 10)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
First job id: 857095,Job id 2: 857096,Job id 3: 857097
## BONGARD_HOI+VASR (aux loss ratio = 10)
sbatch --time=4-00:00:00 --partition=long --mem=36GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
Submitted batch job 857098

## BONGARD_HOI (aux loss ratio = 1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
First job id: 857099,Job id 2: 857100,Job id 3: 857101
## VASR (aux loss ratio = 1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
First job id: 857102,Job id 2: 857103,Job id 3: 857104
## BONGARD_HOI+VASR (aux loss ratio = 1)
sbatch --time=4-00:00:00 --partition=long --mem=36GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
Submitted batch job 857105

## BONGARD_HOI (aux loss ratio = 0.1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
First job id: 857106,Job id 2: 857107,Job id 3: 857108
## VASR (aux loss ratio = 0.1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
First job id: 857109,Job id 2: 857110,Job id 3: 857111
## BONGARD_HOI+VASR (aux loss ratio = 0/1)
sbatch --time=4-00:00:00 --partition=long --mem=36GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
Submitted batch job 857112

### 60 slots - Slot model trained (NOT ALL RAN YET - verify results before running (may not be worth it)

## BONGARD_HOI (aux loss ratio = 10)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
First job id: 857113,Job id 2: 857114,Job id 3: 857115
## VASR (aux loss ratio = 10)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
First job id: 857116,Job id 2: 857117,Job id 3: 857118
## BONGARD_HOI+VASR (aux loss ratio = 10)
sbatch --time=4-00:00:00 --partition=long --mem=36GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
Submitted batch job 857119

## BONGARD_HOI (aux loss ratio = 1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
## VASR (aux loss ratio = 1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
## BONGARD_HOI+VASR (aux loss ratio = 1)
sbatch --time=4-00:00:00 --partition=long --mem=36GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt

## BONGARD_HOI (aux loss ratio = 0.1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
## VASR (aux loss ratio = 0.1)
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
## BONGARD_HOI+VASR (aux loss ratio = 0/1)
sbatch --time=4-00:00:00 --partition=long --mem=36GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=8 model.slot_model.num_slots=60 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt



### 60 slots - Slot model frozen
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring img_size=128 batch_size=8 model.slot_model.num_slots=60 early_stopping_patience=100 max_epochs=500 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
First job id: 850860,Job id 2: 850861,Job id 3: 850862,Job id 4: 850863


## VASR (SLOT=BONGARD_HOI+VASR)
### 20 slots - Slot model frozen
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring img_size=128 batch_size=32 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 845347 # 10GB memory looks good
./scripts/run_bulk_2_0.sh 3 845347 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring img_size=128 batch_size=32 model.slot_model.num_slots=20 +trainer.val_check_interval=0.5
Job based on: 845347,Job id 1: 850850,Job id 2: 850851,Job id 3: 850852

### 60 slots - Slot model frozen
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring img_size=128 batch_size=16 model.slot_model.num_slots=60 +trainer.val_check_interval=0.5 model.slot_model.ckpt_path=/app/model_checkpoints/835822/last.ckpt
First job id: 850853,Job id 2: 850854,Job id 3: 850855,Job id 4: 850856

# Scoring -- WReN
## BONGARD_HOI (SLOT=BONGARD_HOI+VASR)

### 20 slots - Slot model frozen -- averaged
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring img_size=128 batch_size=16 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 853551
./scripts/run_bulk_2_0.sh 3 853551 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring img_size=128 batch_size=16 model.slot_model.num_slots=20 +trainer.val_check_interval=0.5
Job based on: 853551,Job id 1: 853552,Job id 2: 853553,Job id 3: 853554

### 20 slots - Slot model frozen -- order
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring img_size=128 batch_size=16 model.wren_type=order model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 853555
./scripts/run_bulk_2_0.sh 3 853555 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring img_size=128 batch_size=16 wren_type=order model.slot_model.num_slots=20 +trainer.val_check_interval=0.5
Job based on: 853555,Job id 1: 853556,Job id 2: 853557,Job id 3: 853558

### 20 slots - Slot model frozen -- each
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring img_size=128 batch_size=16 model.wren_type=each model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 853559
./scripts/run_bulk_2_0.sh 3 853559 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring img_size=128 batch_size=16 model.wren_type=each model.slot_model.num_slots=20 +trainer.val_check_interval=0.5
Job based on: 853559,Job id 1: 853560,Job id 2: 853561,Job id 3: 853562

## VASR (SLOT=BONGARD_HOI+VASR)

### 20 slots - Slot model frozen -- averaged
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_wren_scoring img_size=128 batch_size=32 model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 853563
./scripts/run_bulk_2_0.sh 3 853563 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_wren_scoring img_size=128 batch_size=32 model.slot_model.num_slots=20 +trainer.val_check_interval=0.5
Job based on: 853563,Job id 1: 853564,Job id 2: 853565,Job id 3: 853566

### 20 slots - Slot model frozen -- order
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_wren_scoring img_size=128 batch_size=32 model.wren_type=order model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 853567
./scripts/run_bulk_2_0.sh 3 853567 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_wren_scoring img_size=128 batch_size=32 model.wren_type=order model.slot_model.num_slots=20 +trainer.val_check_interval=0.5
Job based on: 853567,Job id 1: 853568,Job id 2: 853569,Job id 3: 853570

### 20 slots - Slot model frozen -- each
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_wren_scoring img_size=128 batch_size=32 model.wren_type=each model.slot_model.num_slots=20 model.slot_model.ckpt_path=/app/model_checkpoints/839928/last.ckpt
# 853571
./scripts/run_bulk_2_0.sh 3 853571 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_wren_scoring img_size=128 batch_size=32 model.wren_type=each model.slot_model.num_slots=20 +trainer.val_check_interval=0.5
Job based on: 853571,Job id 1: 853572,Job id 2: 853573,Job id 3: 853574

# Scoring -- feature transformer

## BONGARD HOI (ViT features pooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true log_every_n_steps=200
857367
./scripts/run_bulk_2_0.sh 3 857367 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_trans img_size=128 batch_size=16 model.pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 857367,Job id 1: 857368,Job id 2: 857369,Job id 3: 857370
https://wandb.ai/avr_universal/AVR_universal/runs/6gddq83o

## BONGARD HOI (WReN features pooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans img_size=128 batch_size=16 log_every_n_steps=200
857355
./scripts/run_bulk_2_0.sh 3 857355 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 857355,Job id 1: 857356,Job id 2: 857357,Job id 3: 857358
https://wandb.ai/avr_universal/AVR_universal/runs/dj45oe07

## BONGARD HOI (WReN features unpooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.in_dim=148480 log_every_n_steps=200
857359
./scripts/run_bulk_2_0.sh 3 857359 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.in_dim=148480 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 857359,Job id 1: 857360,Job id 2: 857361,Job id 3: 857362
https://wandb.ai/avr_universal/AVR_universal/runs/5snnqdhk

## VASR (ViT features pooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true log_every_n_steps=200
857371
./scripts/run_bulk_2_0.sh 3 857371 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 857371,Job id 1: 857372,Job id 2: 857373,Job id 3: 857374
https://wandb.ai/avr_universal/AVR_universal/runs/z6jalvu0

## VASR (WReN features pooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 log_every_n_steps=200
857363
./scripts/run_bulk_2_0.sh 3 857363 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 857363,Job id 1: 857364,Job id 2: 857365,Job id 3: 857366
https://wandb.ai/avr_universal/AVR_universal/runs/zdhchdva

## VASR (WReN features unpooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.in_dim=148480 log_every_n_steps=200
857375
./scripts/run_bulk_2_0.sh 3  857375 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.in_dim=148480 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 857375,Job id 1: 857376,Job id 2: 857377,Job id 3: 857378
https://wandb.ai/avr_universal/AVR_universal/runs/1quox1o2

# Images single
## BONGARD_HOI
## Test run
#850767 sbatch --time=0-00:05:00 --mem=32GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_images trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=64 model.num_slots=20

# ### 20 slots
# ./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_images early_stopping_patience=20 batch_size=128 model=stsnv3 model.num_slots=20
# First job id: 850769,Job id 2: 850770,Job id 3: 850771,Job id 4: 850772,Job id 5: 850773
### 20 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_images early_stopping_patience=20 batch_size=64 model=stsnv3 model.num_slots=20
First job id: 870722,Job id 2: 870723,Job id 3: 870724,Job id 4: 870725,Job id 5: 870726

### 10 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_images early_stopping_patience=20 batch_size=128 model=stsnv3 model.num_slots=10
First job id: 850774,Job id 2: 850775,Job id 3: 850776,Job id 4: 850777,Job id 5: 850778

### 30 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_images early_stopping_patience=20 batch_size=32 model=stsnv3 model.num_slots=30
First job id: 850779,Job id 2: 850780,Job id 3: 850781,Job id 4: 850782,Job id 5: 850783

### 40 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_images early_stopping_patience=20 batch_size=32 model=stsnv3 model.num_slots=40
First job id: 850784,Job id 2: 850785,Job id 3: 850786,Job id 4: 850787,Job id 5: 850788

### 50 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_images early_stopping_patience=20 batch_size=32 model=stsnv3 model.num_slots=50
First job id: 850789,Job id 2: 850790,Job id 3: 850791,Job id 4: 850792,Job id 5: 850793

### 60 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_images early_stopping_patience=20 batch_size=16 model=stsnv3 model.num_slots=60
First job id: 850794,Job id 2: 850795,Job id 3: 850796,Job id 4: 850797,Job id 5: 850798


## Test run
#850768 sbatch --time=0-00:05:00 --mem=32GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_images trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=64 model.num_slots=20

# ### 20 slots
# ./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_images early_stopping_patience=40 batch_size=128 model=stsnv3 model.num_slots=20
# First job id: 850799,Job id 2: 850800,Job id 3: 850801,Job id 4: 850802,Job id 5: 850803
### 20 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_images early_stopping_patience=40 batch_size=64 model=stsnv3 model.num_slots=20
First job id: 870727, Job id 2: 870728, Job id 3: 870729, Job id 4: 870730, Job id 5: 870731

### 10 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_images early_stopping_patience=40 batch_size=128 model=stsnv3 model.num_slots=10
First job id: 850804,Job id 2: 850805,Job id 3: 850806,Job id 4: 850807,Job id 5: 850808

### 30 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_images early_stopping_patience=40 batch_size=32 model=stsnv3 model.num_slots=30
First job id: 850809,Job id 2: 850810,Job id 3: 850811,Job id 4: 850812,Job id 5: 850813

### 40 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_images early_stopping_patience=40 batch_size=32 model=stsnv3 model.num_slots=40
First job id: 850814,Job id 2: 850815,Job id 3: 850816,Job id 4: 850817,Job id 5: 850818

### 50 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_images early_stopping_patience=40 batch_size=32 model=stsnv3 model.num_slots=50
First job id: 850819,Job id 2: 850820,Job id 3: 850821,Job id 4: 850822,Job id 5: 850823

### 60 slots
./scripts/run_bulk.sh 5 "--time=1-00:00:00 --mem=32GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_images early_stopping_patience=40 batch_size=16 model=stsnv3 model.num_slots=60
First job id: 850824,Job id 2: 850825,Job id 3: 850826,Job id 4: 850827,Job id 5: 850828

# Scoring
## BONGARD_HOI (SLOT=BONGARD_HOI)

### 20 slots - Slot model frozen
# ./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring img_size=128 batch_size=16 model.slot_model.num_slots=20 early_stopping_patience=100 max_epochs=500 model.slot_model.ckpt_path=/app/model_checkpoints/850773/last.ckpt
# First job id: 870567, Job id 2: 870568, Job id 3: 870569, Job id 4: 870570
./scripts/run_bulk_3_0.sh 4 870726 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring img_size=128 batch_size=16 model.slot_model.num_slots=20 early_stopping_patience=100 max_epochs=500 model.slot_model.ckpt_path=/app/model_checkpoints/870726/last.ckpt
Job runned after: 870726,Job id 1: 870733,Job id 2: 870734,Job id 3: 870735,Job id 4: 870736
### 60 slots - Slot model frozen
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring img_size=128 batch_size=8 model.slot_model.num_slots=60 early_stopping_patience=100 max_epochs=500 model.slot_model.ckpt_path=/app/model_checkpoints/850798/last.ckpt
First job id: 870571, Job id 2: 870572, Job id 3: 870573, Job id 4: 870574

## VASR (SLOT=VASR)
### 20 slots - Slot model frozen
#./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring img_size=128 batch_size=32 model.slot_model.num_slots=20 +trainer.val_check_interval=0.5 model.slot_model.ckpt_path=/app/model_checkpoints/850803/last.ckpt
#First job id: 870575, Job id 2: 870576, Job id 3: 870577, Job id 4: 870578
./scripts/run_bulk_3_0.sh 4 870731 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring img_size=128 batch_size=32 model.slot_model.num_slots=20 +trainer.val_check_interval=0.5 model.slot_model.ckpt_path=/app/model_checkpoints/870731/last.ckpt
Job runned after: 870731, Job id 1: 870737, Job id 2: 870738, Job id 3: 870739, Job id 4: 870740
### 60 slots - Slot model frozen
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring img_size=128 batch_size=16 model.slot_model.num_slots=60 +trainer.val_check_interval=0.5 model.slot_model.ckpt_path=/app/model_checkpoints/850828/last.ckpt
First job id: 870579, Job id 2: 870580, Job id 3: 870581, Job id 4: 870582


## BONGARD_HOI (aux loss ratio = 0.1)
# ./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/850773/last.ckpt
# First job id: 870583, Job id 2: 870584, Job id 3: 870585
./scripts/run_bulk_3_0.sh 3 870726 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/870726/last.ckpt
Job runned after: 870726,Job id 1: 870741,Job id 2: 870742,Job id 3: 870743
## VASR (aux loss ratio = 0.1)
#./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/850803/last.ckpt
#First job id: 870586, Job id 2: 870587, Job id 3: 870588
./scripts/run_bulk_3_0.sh 3 870731 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=0.1 model.slot_model.ckpt_path=/app/model_checkpoints/870731/last.ckpt
Job runned after: 870731, Job id 1: 870744, Job id 2: 870745, Job id 3: 870746
## BONGARD_HOI (aux loss ratio = 1)
# ./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/850773/last.ckpt
# First job id: 870589, Job id 2: 870590, Job id 3: 870591
./scripts/run_bulk_3_0.sh 3 870726 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/870726/last.ckpt
Job runned after: 870726, Job id 1: 870747, Job id 2: 870748, Job id 3: 870749
## VASR (aux loss ratio = 1)
#./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/850803/last.ckpt
#First job id: 870592, Job id 2: 870593, Job id 3: 870594
./scripts/run_bulk_3_0.sh 3 870731 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=1 model.slot_model.ckpt_path=/app/model_checkpoints/870731/last.ckpt
Job runned after: 870731, Job id 1: 870750, Job id 2: 870751, Job id 3: 870752

## BONGARD_HOI (aux loss ratio = 10)
# ./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/850773/last.ckpt
# First job id: 870595, Job id 2: 870596, Job id 3: 870597
./scripts/run_bulk_3_0.sh 3 870726 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_full_train img_size=128 batch_size=16 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/870726/last.ckpt
Job runned after: 870726, Job id 1: 870753, Job id 2: 870754, Job id 3: 870755

## VASR (aux loss ratio = 10)
#./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/850803/last.ckpt
#First job id: 870598, Job id 2: 870599, Job id 3: 870600
# TODO: run me
./scripts/run_bulk_3_0.sh 3 870731 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_full_train img_size=128 batch_size=32 model.slot_model.num_slots=20 model.auxiliary_loss_ratio=10 model.slot_model.ckpt_path=/app/model_checkpoints/870731/last.ckpt
Job runned after: 870731, Job id 1: 870756, Job id 2: 870757, Job id 3: 870758

### Abstract shapes bongard logo + vaec::

# test
# 20 slots
sbatch --time=0-00:10:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_vaec_images trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=128 model.num_slots=20
# 871033 80x256 ERROR (im_size x batch size) https://wandb.ai/avr_universal/AVR_universal/runs/ogwm0yd0
# 871034 80x128 OK (im_size x batch size) https://wandb.ai/avr_universal/AVR_universal/runs/vnb7hk48

# 10 slots
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_vaec_images trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=256 model.num_slots=10
# 871035 80x256 OK https://wandb.ai/avr_universal/AVR_universal/runs/msnwqpjj

# 20 slots logo
sbatch --time=0-00:10:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_images trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=256 model.num_slots=20
# 10 slots logo
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_images trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=256 model.num_slots=10


## both
# 10 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_images img_size=80 batch_size=256 model=stsnv3 model.num_slots=10
First job id: 871039, Job id 2: 871040, Job id 3: 871041

# 20 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_images img_size=80 batch_size=128 model=stsnv3 model.num_slots=20
First job id: 871036, Job id 2: 871037, Job id 3: 871038

## single
### BONGARD LOGO
# 10 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_images img_size=80 batch_size=256 model=stsnv3 model.num_slots=10
First job id: 871043, Job id 2: 871044, Job id 3: 871045

# 20 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_images img_size=80 batch_size=128 model=stsnv3 model.num_slots=20
First job id: 871046, Job id 2: 871047, Job id 3: 871048


### VAEC
# 10 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_images img_size=80 batch_size=256 model=stsnv3 model.num_slots=10
First job id: 871049, Job id 2: 871050, Job id 3: 871051

# 20 slots
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=16GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_images img_size=80 batch_size=128 model=stsnv3 model.num_slots=20
First job id: 871052, Job id 2: 871053, Job id 3: 871054

### Scoring Abstract - tst
### Based on both, trained on single - frozen

### 10 bongard_logo
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_scoring trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 59%
### 20 bongard_logo
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_scoring trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 97%

### 10 vaec
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vaec_scoring trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 872966 87-93%
### 20 vaec
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vaec_scoring trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 872967 85-95%


### Scoring Abstract
### Based on both, trained on single - frozen

### 10 bongard_logo
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
First job id: 872969, Job id 2: 872970
./scripts/run_bulk_2_0.sh 2 872970 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
Job based on: 872970,Job id 1: 874323,Job id 2: 874324
### 20 bongard_logo
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
First job id: 872971, Job id 2: 872972
./scripts/run_bulk_2_0.sh 2 872972 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
Job based on: 872972, Job id 1: 874325, Job id 2: 874326

### 10 vaec
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
First job id: 872973, Job id 2: 872974
./scripts/run_bulk_2_0.sh 2 872974 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
Job based on: 872974, Job id 1: 874327, Job id 2: 874328

### 20 vaec
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
First job id: 872975, Job id 2: 872976
./scripts/run_bulk_2_0.sh 2 872976 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
Job based on: 872976, Job id 1: 874330, Job id 2: 874331

### based on single
# 10 LOGO: model_checkpoints/871045/epoch=158-step=80931.ckpt
# 20 LOGO: model_checkpoints/871046/epoch=56-step=58026.ckpt
### 10 bongard_logo
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'"
First job id: 872977, Job id 2: 872978
./scripts/run_bulk_2_0.sh 1 872978 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'"
Job based on: 872978, Job id 1: 874334

### 20 bongard_logo
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'"
First job id: 872979, Job id 2: 872980
./scripts/run_bulk_2_0.sh 2 872980 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'"
Job based on: 872980, Job id 1: 874335, Job id 2: 874336

# 10 VAEC: model_checkpoints/871049/epoch=25-step=13546.ckpt
# 20 VAEC: model_checkpoints/871052/epoch=12-step=13546.ckpt
### 10 vaec
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'"
First job id: 872981, Job id 2: 872982
./scripts/run_bulk_2_0.sh 2 872982 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'"
Job based on: 872982, Job id 1: 874340, Job id 2: 874341

### 20 vaec
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'"
First job id: 872983, Job id 2: 872984
./scripts/run_bulk_2_0.sh 2 872984 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'"
Job based on: 872984, Job id 1: 874342, Job id 2: 874343

### bongard_logo + vaec
### 10
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 873668
./scripts/run_bulk.sh 2 "--time=1-00:00:00 --mem=40GB --gpus=2" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
First job id: 873670, Job id 2: 873671
./scripts/run_bulk_2_0.sh 2 873671 "--time=1-00:00:00 --mem=40GB --gpus=2" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring early_stopping_patience=500 img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
Job based on: 873671, Job id 1: 874344, Job id 2: 874345

### 20
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 873669
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
First job id: 873687, Job id 2: 873688 #, Job id 3: 873689, Job id 4: 873690
./scripts/run_bulk_2_0.sh 2 873688 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring early_stopping_patience=500 img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
Job based on: 873688, Job id 1: 874346, Job id 2: 874347

### Scoring Abstract
### Based on both, train slot model
### 10 bongard_logo
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_scoring ++model.freeze_slot_model=false trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 873722
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
First job id: 873738, Job id 2: 873739, Job id 3: 873740
./scripts/run_bulk_2_0.sh 2 873740 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
Job based on: 873740, Job id 1: 874350, Job id 2: 874351
### 20 bongard_logo
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_scoring ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 873726
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
First job id: 873741, Job id 2: 873742, Job id 3: 873743
./scripts/run_bulk_2_0.sh 2 873743 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
Job based on: 873743, Job id 1: 874352, Job id 2: 874353

### 10 vaec
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vaec_scoring ++model.freeze_slot_model=false trainer=test img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 873733
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
First job id: 873744, Job id 2: 873745, Job id 3: 873746
./scripts/run_bulk_2_0.sh 2 873744 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
Job based on: 873744, Job id 1: 874354, Job id 2: 874355

### 20 vaec
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vaec_scoring ++model.freeze_slot_model=false trainer=test img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 873734
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
First job id: 873748, Job id 2: 873749, Job id 3: 873750
./scripts/run_bulk_2_0.sh 2 873750 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
Job based on: 873750, Job id 1: 874358, Job id 2: 874359

### Based on single, train slot model
### 10 bongard_logo
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'"
First job id: 873751, Job id 2: 873752, Job id 3: 873753
./scripts/run_bulk_2_0.sh 2 873753 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'"
Job based on: 873753, Job id 1: 874360, Job id 2: 874361

### 20 bongard_logo
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'"
First job id: 873754, Job id 2: 873755, Job id 3: 873756
./scripts/run_bulk_2_0.sh 2 873756 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'"
Job based on: 873756, Job id 1: 874362, Job id 2: 874363

### 10 vaec
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'"
First job id: 873757, Job id 2: 873758, Job id 3: 873759
./scripts/run_bulk_2_0.sh 2 873759 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'"
Job based on: 873759, Job id 1: 874364, Job id 2: 874365

### 20 vaec
./scripts/run_bulk.sh 3 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'"
First job id: 873760, Job id 2: 873761, Job id 3: 873762
./scripts/run_bulk_2_0.sh 2 873762 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'"
Job based on: 873762, Job id 1: 874366, Job id 2: 874367

### bongard_logo + vaec, train slot model
### 10
#sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
First job id: 873763, Job id 2: 873764, Job id 3: 873765, Job id 4: 873766

### 20
#sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false trainer=test every_n_train_steps=50 every_n_epochs=null img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
First job id: 873767, Job id 2: 873768, Job id 3: 873769, Job id 4: 873770
./scripts/run_bulk_2_0.sh 2 873770 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
Job based on: 873770, Job id 1: 874368, Job id 2: 874369


### Unpooled features

## BONGARD HOI (WReN features unpooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.use_detection=true model.in_dim=148480 log_every_n_steps=200
876440
./scripts/run_bulk_2_0.sh 3 876440 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.use_detection=true model.in_dim=148480 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 876440, Job id 1: 876441, Job id 2: 876442, Job id 3: 876443
https://wandb.ai/avr_universal/AVR_universal/runs/xb2rvf7l

## VASR (WReN features unpooled)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.use_detection=true model.in_dim=148480 log_every_n_steps=200
876444
./scripts/run_bulk_2_0.sh 3  876444 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=8 model.wren_type=vit model.use_detection=true model.in_dim=148480 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 876444, Job id 1: 876445, Job id 2: 876446, Job id 3: 876447
https://wandb.ai/avr_universal/AVR_universal/runs/3ck0cyap


### transformer parameters

## VASR (ViT features pooled) smaller transformer (depth 12)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.depth=12
876136
./scripts/run_bulk_2_0.sh 3 876136 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.depth=12
Job based on: 876136, Job id 1: 876137, Job id 2: 876138, Job id 3: 876139
https://wandb.ai/avr_universal/AVR_universal/runs/xp0zkkxz

## VASR (ViT features pooled) smaller transformer (depth 18)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.depth=18
876999
./scripts/run_bulk_2_0.sh 3 876999 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.depth=18
Job based on: 876999, Job id 1: 877000, Job id 2: 877001, Job id 3: 877002

## VASR (ViT features pooled) smaller transformer (depth 30) 
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.depth=30
877003
./scripts/run_bulk_2_0.sh 3 877003 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.depth=30
Job based on: 877003, Job id 1: 877004, Job id 2: 877005, Job id 3: 877006

## VASR (ViT features pooled) smaller transformer (dim)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.mlp_dim=256
876140
./scripts/run_bulk_2_0.sh 3 876140 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.mlp_dim=256
Job based on: 876140, Job id 1: 876141, Job id 2: 876142, Job id 3: 876143
https://wandb.ai/avr_universal/AVR_universal/runs/wht2whf3

## VASR (ViT features pooled) smaller transformer (dim 1024)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.mlp_dim=1024
876470
./scripts/run_bulk_2_0.sh 3 876470 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.mlp_dim=1024
Job based on: 876470, Job id 1: 876471, Job id 2: 876472, Job id 3: 876473
https://wandb.ai/avr_universal/AVR_universal/runs/bfd61zn7

## VASR (ViT features pooled) smaller transformer (heads 4)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.heads=4
876144
./scripts/run_bulk_2_0.sh 3 876144 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.heads=4
Job based on: 876144, Job id 1: 876145, Job id 2: 876146, Job id 3: 876147
https://wandb.ai/avr_universal/AVR_universal/runs/7myttor4

## VASR (ViT features pooled) smaller transformer (heads 6)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.heads=6
876474
./scripts/run_bulk_2_0.sh 3 876474 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.heads=6
Job based on: 876144, Job id 1: 876145, Job id 2: 876146, Job id 3: 876147
https://wandb.ai/avr_universal/AVR_universal/runs/8e0dua9s

## VASR (ViT features pooled) smaller transformer (heads 10)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true early_stopping_patience=25 model.transformer.heads=10
877007
./scripts/run_bulk_2_0.sh 3 877007 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_scoring_trans img_size=128 batch_size=16 model.pooling=true model.pos_emb.feature_pooling=true +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.transformer.heads=10
Job based on: 877007, Job id 1: 877008, Job id 2: 877009, Job id 3: 877010

### Smaller WReN depth

## VASR (WReN features pooled) smaller wren depth
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 log_every_n_steps=200 early_stopping_patience=25 model.g_depth=1 model.f_depth=1
877016
./scripts/run_bulk_2_0.sh 3 877016 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 model.g_depth=1 model.f_depth=1
Job based on: 877016, Job id 1: 877017, Job id 2: 877018, Job id 3: 877019

### Simple model (from VASR article)

## BONGARD HOI (simple model)
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans model.wren_type=basic img_size=128 batch_size=16 log_every_n_steps=200
877011
./scripts/run_bulk_2_0.sh 3 877011 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_wren_scoring_trans model.wren_type=basic img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 877011, Job id 1: 877012, Job id 2: 877013, Job id 3: 877014

## VASR (simple model) 
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_wren_scoring_trans model.wren_type=basic img_size=128 batch_size=16 log_every_n_steps=200
876128
./scripts/run_bulk_2_0.sh 3 876128 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_wren_scoring_trans model.wren_type=basic img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200
Job based on: 876128, Job id 1: 876129, Job id 2: 876130, Job id 3: 876131
https://wandb.ai/avr_universal/AVR_universal/runs/tha0vupw

## VASR (simple model) only cap
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans_captions model.wren_type=basic img_size=128 batch_size=16 log_every_n_steps=200 model.use_captions=true early_stopping_patience=25
876257
./scripts/run_bulk_2_0.sh 3 876257 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans_captions img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200 model.use_captions=true early_stopping_patience=25
Job based on: 876257, Job id 1: 876258, Job id 2: 876259, Job id 3: 876260
https://wandb.ai/avr_universal/AVR_universal/runs/9wwlib19


### learning rate tests

## VASR (WReN features pooled) higher lr
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 log_every_n_steps=200 early_stopping_patience=25 lr=0.001
876120
./scripts/run_bulk_2_0.sh 3 876120 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 lr=0.001
Job based on: 876120, Job id 1: 876121, Job id 2: 876122, Job id 3: 876123
https://wandb.ai/avr_universal/AVR_universal/runs/ticze5ac

## VASR (WReN features pooled) lower lr
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 log_every_n_steps=200 lr=0.00001
876124
./scripts/run_bulk_2_0.sh 3 876124 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run_yolo.sh src/main.py +experiment=vasr_wren_scoring_trans img_size=128 batch_size=16 +trainer.val_check_interval=0.5 log_every_n_steps=200 lr=0.00001
Job based on: 876124, Job id 1: 876125, Job id 2: 876126, Job id 3: 876127
https://wandb.ai/avr_universal/AVR_universal/runs/q9qtish7


### Relational matrix model with MLP
## combined bongard hoi bongard logo
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_bongard_logo_combined img_size=128 batch_size=8 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" log_every_n_steps=200 early_stopping_patience=25 
882190
./scripts/run_bulk_2_0.sh 3  879822 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_bongard_logo_combined img_size=128 batch_size=8 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 
Job based on: 879648, Job id 1: 879673, Job id 2: 879674, Job id 3: 879675
https://wandb.ai/avr_universal/AVR_universal/runs/4emqo9a8

## combined vasr vaec
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_vaec_combined img_size=128 batch_size=16 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" log_every_n_steps=200 early_stopping_patience=25 
879649
./scripts/run_bulk_2_0.sh 3  879649 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_vaec_combined img_size=128 batch_size=16 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25
Job based on: 879649, Job id 1: 879676, Job id 2: 879677, Job id 3: 879678
https://wandb.ai/avr_universal/AVR_universal/runs/cgqwqk6x

## only bongard hoi
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_combined img_size=128 batch_size=8 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" log_every_n_steps=200 early_stopping_patience=25 
882199
./scripts/run_bulk_2_0.sh 3  879659 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_combined img_size=128 batch_size=8 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25 
Job based on: 879659, Job id 1: 879667, Job id 2: 879668, Job id 3: 879669
https://wandb.ai/avr_universal/AVR_universal/runs/ux2b5hrx

## only vasr
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_combined img_size=128 batch_size=16 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" log_every_n_steps=200 early_stopping_patience=25 
879660
./scripts/run_bulk_2_0.sh 3  879660 "--time=1-00:00:00 --mem=24GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_combined img_size=128 batch_size=16 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" +trainer.val_check_interval=0.5 log_every_n_steps=200 early_stopping_patience=25
Job based on: 879660, Job id 1: 879670, Job id 2: 879671, Job id 3: 879672

### Relational matrix model with transformer
## only bongard hoi
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_combined_transformer img_size=128 batch_size=8 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" model.relational_activation_func='none' model.scoring_transformer.dim=4 log_every_n_steps=200 early_stopping_patience=25
894042
https://wandb.ai/avr_universal/AVR_universal/runs/tld2fp23

## only vasr 
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_combined_transformer img_size=128 batch_size=8 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" model.relational_activation_func='none' model.scoring_transformer.dim=4 log_every_n_steps=200 early_stopping_patience=25
897028

## combined bongard hoi bongard logo
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 /home2/faculty/akaminski/Universal_AVR_Model/scripts/run.sh /home2/faculty/akaminski/Universal_AVR_Model/src/main.py +experiment=bongard_hoi_combined img_size=128 batch_size=8 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" log_every_n_steps=200 early_stopping_patience=25
897029




# w prostych przypadkach (tam gdzie tworzyles opisy,... - nowe zbiory z nowymi configami bedziesz musial stworzyc nowe yamle dla bongardowych zbiorow (żeby zadziałało zapisywane w tym samym runie w wandb i nie nadpisywało wyników, alternatywa zmiana logowanie w AVRModule ale wtedy wszystkie istniejace wyniki w wandb beda mialy inna strukture niz to co wprowadzimy)
# HOI:
- data.tasks=[bongard_hoi_seen-seen]
- data.tasks=[bongard_hoi_seen-seen]
- data.tasks=[bongard_hoi_seen-unseen]
- data.tasks=[bongard_hoi_unseen-seen]
- data.tasks=[bongard_hoi_unseen-unseen]
# LOGO:
- data.tasks=[bongard_logo_test_bd]
- data.tasks=[bongard_logo_test_ff]
- data.tasks=[bongard_logo_test_hd_comb]
- data.tasks=[bongard_logo_test_hd_novel]
# VAEC:
- data.tasks=[vaec_test1]
- data.tasks=[vaec_test2]
- data.tasks=[vaec_test3]
- data.tasks=[vaec_test4]
- data.tasks=[vaec_test5]
# VASR:
- data.tasks=[vasr]




/app/model_checkpoints/845333/epoch=20-step=30261.ckpt: bongard_hoi
/app/model_checkpoints/857092/epoch=21-step=31702.ckpt: bongard_hoi
/app/model_checkpoints/857095/epoch=1-step=24084.ckpt: vasr
/app/model_checkpoints/857099/epoch=13-step=20174.ckpt: bongard_hoi
/app/model_checkpoints/857102/epoch=0-step=12042.ckpt: vasr
/app/model_checkpoints/857106/epoch=21-step=31702.ckpt: bongard_hoi
/app/model_checkpoints/857109/epoch=0-step=12042.ckpt: vasr
/app/model_checkpoints/857115/epoch=23-step=69144.ckpt: bongard_hoi
/app/model_checkpoints/850863/epoch=66-step=193027.ckpt: bongard_hoi
/app/model_checkpoints/845347/epoch=0-step=12042.ckpt: vasr
/app/model_checkpoints/850854/epoch=2-step=72252.ckpt: vasr
/app/model_checkpoints/870733/epoch=17-step=25938.ckpt: bongard_hoi
/app/model_checkpoints/870574/epoch=43-step=126764.ckpt: bongard_hoi
/app/model_checkpoints/870737/epoch=0-step=6021.ckpt: vasr
/app/model_checkpoints/870579/epoch=0-step=12042.ckpt: vasr
/app/model_checkpoints/870741/epoch=8-step=12969.ckpt: bongard_hoi
/app/model_checkpoints/870744/epoch=0-step=12042.ckpt: vasr
/app/model_checkpoints/870747/epoch=17-step=25938.ckpt: bongard_hoi
/app/model_checkpoints/870750/epoch=0-step=12042.ckpt: vasr
/app/model_checkpoints/870753/epoch=0-step=1441.ckpt: bongard_hoi
/app/model_checkpoints/870756/epoch=0-step=12042.ckpt: vasr
/app/model_checkpoints/872969/epoch=311-step=45552.ckpt: bongard_logo
/app/model_checkpoints/874325/epoch=439-step=128040.ckpt: bongard_logo
/app/model_checkpoints/872973/epoch=22-step=3427.ckpt: vaec
/app/model_checkpoints/872975/epoch=1-step=596.ckpt: vaec
/app/model_checkpoints/872978/epoch=362-step=52998.ckpt: bongard_logo
/app/model_checkpoints/872980/epoch=204-step=59655.ckpt: bongard_logo
/app/model_checkpoints/872981/epoch=37-step=5662.ckpt: vaec
/app/model_checkpoints/872983/epoch=59-step=17880.ckpt: vaec
/app/model_checkpoints/873670/epoch=18-step=2831.ckpt: vaec, bongard_logo
/app/model_checkpoints/873687/epoch=45-step=27370.ckpt: vaec, bongard_logo
/app/model_checkpoints/873738/epoch=8-step=1314.ckpt: bongard_logo
/app/model_checkpoints/873741/epoch=56-step=16587.ckpt: bongard_logo
/app/model_checkpoints/873748/epoch=19-step=5960.ckpt: vaec
/app/model_checkpoints/873751/epoch=0-step=146.ckpt: bongard_logo
/app/model_checkpoints/873754/epoch=2-step=873.ckpt: bongard_logo
/app/model_checkpoints/873757/epoch=38-step=5811.ckpt: vaec
/app/model_checkpoints/873760/epoch=61-step=18476.ckpt: vaec
/app/model_checkpoints/873763/epoch=63-step=19072.ckpt: vaec, bongard_logo
/app/model_checkpoints/873767/epoch=1-step=1190.ckpt: vaec, bongard_logo

LAST_ID=1
for task_nm in bongard_hoi_seen-seen bongard_hoi_seen-unseen bongard_hoi_unseen-seen bongard_hoi_unseen-unseen; do
    LAST_ID=$(sbatch --parsable --time=0-01:00:00 --dependency=afterany:$LAST_ID scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/870733/epoch=17-step=25938.ckpt'" data/tasks=[${task_nm}])
    echo -n "${LAST_ID} "
done
# 882623 882624 882625 882626

LAST_ID=1 # dummy id to simplify the code
for task_nm in vaec_test1 vaec_test2 vaec_test3 vaec_test4 vaec_test5; do
    LAST_ID=$(sbatch --parsable --time=0-01:00:00 --dependency=afterany:$LAST_ID scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/873760/epoch=61-step=18476.ckpt'" data/tasks=[${task_nm}])
    echo -n "${LAST_ID} "
done
# 882588, 882589, 882590, 882591, 882592

#sbatch --time=0-01:00:00 scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/873760/epoch=61-step=18476.ckpt'" data/tasks=[vaec]
# 882440 #OK

LAST_ID=1 # dummy id to simplify the code
for task_nm in vaec_test1 vaec_test2 vaec_test3 vaec_test4 vaec_test5; do
    LAST_ID=$(sbatch --parsable --time=0-01:00:00 --dependency=afterany:$LAST_ID scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/873748/epoch=19-step=5960.ckpt'" data/tasks=[${task_nm}])
    echo -n "${LAST_ID} "
done
# 882593 882594 882595 882596 882597

#sbatch --time=0-01:00:00 scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/873748/epoch=19-step=5960.ckpt'" data/tasks=[vaec]
# 882441 #OK


LAST_ID=1 # dummy id to simplify the code
for task_nm in vaec_test1 vaec_test2 vaec_test3 vaec_test4 vaec_test5; do
    LAST_ID=$(sbatch --parsable --time=0-01:00:00 --dependency=afterany:$LAST_ID scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/873767/epoch=1-step=1190.ckpt'" data/tasks=[${task_nm}] +increment_dataloader_idx=1)
    echo -n "${LAST_ID} "
done
# 882658 882659 882660 882661 882662
# DONE: error with shapes, fix me (propably problem with fact that model had 2 paths and we are using the wrong one (code below worked with different dataset (hard to adjust without changes to the code :/)
# quick fix - passing 2 tasks at the same time but not sure how would it work, maybe parameter to scoring model increment_dataloader_idx (default = 0)  
# added increment_dataloader_idx to scoring_model_v1 and adjusted test.py (to allow to pass this parameter)

# sbatch --time=0-01:00:00 scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/873767/epoch=1-step=1190.ckpt'" data/tasks=[vaec]
# 882442

# Dependecies added only to avoid writing to same wandb resource (not sure if it can cause inconsistencies
LAST_ID=882662
for task_nm in bongard_logo_test_bd bongard_logo_test_ff bongard_logo_test_hd_comb bongard_logo_test_hd_novel; do
    LAST_ID=$(sbatch --parsable --time=0-01:00:00 --dependency=afterany:$LAST_ID scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/873767/epoch=1-step=1190.ckpt'" data/tasks=[${task_nm}])
    echo -n "${LAST_ID} "
done
# 882603 882604 882605 882606

LAST_ID=1
for task_nm in bongard_logo_test_bd bongard_logo_test_ff bongard_logo_test_hd_comb bongard_logo_test_hd_novel; do
    LAST_ID=$(sbatch --parsable --time=0-01:00:00 --dependency=afterany:$LAST_ID scripts/run.sh src/test.py "checkpoint_path='/app/model_checkpoints/872978/epoch=362-step=52998.ckpt'" data/tasks=[${task_nm}])
    echo -n "${LAST_ID} "
done
# 882611 882612 882613 882614


# 1st argument - checkpoing_path,
# 2nd argument - slurm_id dependency (default=1)
# 3rd argument - additional hydra config (e.g. +increment_dataloader_idx=1)
test_prepare () {
    CHECKPOINT_PATH=$1
    LAST_ID=${2:-1}
    ADDITIONAL_PARAMS=${3}
    echo "Params:"
    echo $CHECKPOINT_PATH
    echo $LAST_ID
    echo $ADDITIONAL_PARAMS
}

# arugments - any number of tasks to run
test_run () {
    echo "Slurm ids:"
    for task_nm in ${@}; do
        LAST_ID=$(sbatch --parsable --time=0-00:30:00 --dependency=afterany:${LAST_ID} scripts/run.sh src/test.py "checkpoint_path='${CHECKPOINT_PATH}'" data/tasks=[${task_nm}] ${ADDITIONAL_PARAMS})
        echo -n "${LAST_ID} "
    done
    echo
}

test_bongard_logo () {
    test_prepare $1 $2 $3

    test_run bongard_logo_test_bd bongard_logo_test_ff bongard_logo_test_hd_comb bongard_logo_test_hd_novel
}

test_vaec () {
    test_prepare $1 $2 $3

    test_run vaec_test1 vaec_test2 vaec_test3 vaec_test4 vaec_test5
}

test_bongard_hoi () {
    test_prepare $1 $2 $3

    test_run bongard_hoi_seen-seen bongard_hoi_seen-unseen bongard_hoi_unseen-seen bongard_hoi_unseen-unseen
}

test_vasr () {
    test_prepare $1 $2 $3

    test_run vasr
}


test_vasr /app/model_checkpoints/870756/epoch=0-step=12042.ckpt
# 883409

test_bongard_hoi /app/model_checkpoints/845333/epoch=20-step=30261.ckpt
# 883420 883421 883422 883423
test_bongard_hoi /app/model_checkpoints/857092/epoch=21-step=31702.ckpt
# 883424 883425 883426 883427
test_vasr /app/model_checkpoints/857095/epoch=1-step=24084.ckpt
# 883428
test_bongard_hoi /app/model_checkpoints/857099/epoch=13-step=20174.ckpt
# 883429 883430 883431 883432
test_vasr /app/model_checkpoints/857102/epoch=0-step=12042.ckpt
# 883433
test_bongard_hoi /app/model_checkpoints/857106/epoch=21-step=31702.ckpt
# 883434 883435 883436 883437
test_vasr /app/model_checkpoints/857109/epoch=0-step=12042.ckpt
# 883438
test_bongard_hoi /app/model_checkpoints/857115/epoch=23-step=69144.ckpt
# 883439 883440 883441 883442
test_bongard_hoi /app/model_checkpoints/850863/epoch=66-step=193027.ckpt
# 883443 883444 883445 883446
test_vasr /app/model_checkpoints/845347/epoch=0-step=12042.ckpt
# 883447
test_vasr /app/model_checkpoints/850854/epoch=2-step=72252.ckpt
# 883448
test_bongard_hoi /app/model_checkpoints/870733/epoch=17-step=25938.ckpt
# 883449 883450 883451 883452
test_bongard_hoi /app/model_checkpoints/870574/epoch=43-step=126764.ckpt
# 883453 883454 883455 883456
test_vasr /app/model_checkpoints/870737/epoch=0-step=6021.ckpt
# 883457
test_vasr /app/model_checkpoints/870579/epoch=0-step=12042.ckpt
# 883458
test_bongard_hoi /app/model_checkpoints/870741/epoch=8-step=12969.ckpt
# 883459 883460 883461 883462
test_vasr /app/model_checkpoints/870744/epoch=0-step=12042.ckpt
# 883463
test_bongard_hoi /app/model_checkpoints/870747/epoch=17-step=25938.ckpt
# 883464 883465 883466 883467
test_vasr /app/model_checkpoints/870750/epoch=0-step=12042.ckpt
# 883468
test_bongard_hoi /app/model_checkpoints/870753/epoch=0-step=1441.ckpt
# 883469 883470 883471 883472
test_bongard_logo /app/model_checkpoints/872969/epoch=311-step=45552.ckpt
# 883473 883474 883475 883476
test_bongard_logo /app/model_checkpoints/874325/epoch=439-step=128040.ckpt
# 883477 883478 883479 883480
test_vaec /app/model_checkpoints/872973/epoch=22-step=3427.ckpt
# 883481 883482 883483 883484 883485
test_vaec /app/model_checkpoints/872975/epoch=1-step=596.ckpt
# 883486 883487 883488 883489 883490
test_bongard_logo /app/model_checkpoints/872980/epoch=204-step=59655.ckpt
# 883491 883492 883493 883494
test_vaec /app/model_checkpoints/872981/epoch=37-step=5662.ckpt
# 883495 883496 883497 883498 883499
test_vaec /app/model_checkpoints/872983/epoch=59-step=17880.ckpt
# 883500 883501 883502 883503 883504


test_bongard_logo /app/model_checkpoints/873670/epoch=18-step=2831.ckpt
# 883505 883506 883507 883508
test_vaec /app/model_checkpoints/873670/epoch=18-step=2831.ckpt $LAST_ID "+increment_dataloader_idx=1"
# 883509 883510 883511 883512 883513

test_bongard_logo /app/model_checkpoints/873687/epoch=45-step=27370.ckpt
# 883514 883515 883516 883517
test_vaec /app/model_checkpoints/873687/epoch=45-step=27370.ckpt $LAST_ID "+increment_dataloader_idx=1"
# 883518 883519 883520 883521 883522

test_bongard_logo /app/model_checkpoints/873738/epoch=8-step=1314.ckpt
# 883523 883524 883525 883526
test_bongard_logo /app/model_checkpoints/873741/epoch=56-step=16587.ckpt
# 883527 883528 883529 883530
test_bongard_logo /app/model_checkpoints/873751/epoch=0-step=146.ckpt
# 883531 883532 883533 883534
test_bongard_logo /app/model_checkpoints/873754/epoch=2-step=873.ckpt
# 883535 883536 883537 883538
test_vaec /app/model_checkpoints/873757/epoch=38-step=5811.ckpt
# 883539 883540 883541 883542 883543

test_bongard_logo /app/model_checkpoints/873763/epoch=63-step=19072.ckpt
# 883544 883545 883546 883547
test_vaec /app/model_checkpoints/873763/epoch=63-step=19072.ckpt $LAST_ID "+increment_dataloader_idx=1"
# 883548 883549 883550 883551 883552



### Scoring Abstract
### Based on both, train slot model
### 10 bongard_logo
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=1
# 883848, 883849, 883850, 883851

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=100
# 883852, 883853, 883854, 883855

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=1000
# 883856, 883857, 883858, 883859


### 20 bongard_logo
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=1
# 883860, 883861, 883862, 883863

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=100
# 883864, 883865, 883866, 883867

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=1000
# 883868, 883869, 883870, 883871



### 10 vaec
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=1
# 883873, 883874, 883875, 883876

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=100
# 883877, 883878, 883879, 883880

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=1000
# 883881, 883882, 883883, 883884


### 20 vaec
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=1
# 883885, 883886, 883887, 883888

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=100
# 883889, 883890, 883891, 883892

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=1000
# 883893, 883894, 883895, 883896



### Based on single, train slot model
### 10 bongard_logo
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'" model.auxiliary_loss_ratio=1
# 883774, 883775, 883776, 883777

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'" model.auxiliary_loss_ratio=100
# 883779, 883780, 883781, 883782

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'" model.auxiliary_loss_ratio=1000
# 883783, 883784, 883785, 883786


### 20 bongard_logo
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'" model.auxiliary_loss_ratio=1
# 883787, 883788, 883789, 883790

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'" model.auxiliary_loss_ratio=100
# 883791, 883792, 883793, 883794

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'" model.auxiliary_loss_ratio=1000
# 883795, 883796, 883797, 883798

### 10 vaec
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'" model.auxiliary_loss_ratio=1
# 883799, 883800, 883801, 883802

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'" model.auxiliary_loss_ratio=100
# 883803, 883804, 883805, 883806

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=128 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'" model.auxiliary_loss_ratio=1000
# 883807, 883808, 883809, 883810


### 20 vaec
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" model.auxiliary_loss_ratio=1
# 883811, 883812, 883813, 883814

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" model.auxiliary_loss_ratio=100
# 883815, 883816, 883817, 883818

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring early_stopping_patience=500 ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" model.auxiliary_loss_ratio=1000
# 883819, 883820, 883821, 883822


### bongard_logo + vaec, train slot model
### 10
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=1
# 883823, 883824, 883825, 883826

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=100
# 883827, 883828, 883829, 883830

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=64 model.slot_model.num_slots=10 "model.slot_model.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'" model.auxiliary_loss_ratio=1000
# 883831, 883832, 883833, 883834



### 20
./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=1
# 883835, 883836, 883837, 883838

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=100
# 883839, 883840, 883841, 883842

./scripts/run_bulk.sh 4 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_vaec_scoring ++model.freeze_slot_model=false img_size=80 batch_size=32 model.slot_model.num_slots=20 "model.slot_model.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'" model.auxiliary_loss_ratio=1000
# 883843, 883844, 883845, 883846



### ESNB - tests

# bongards
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=64 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 893835

sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_esnb trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=8
# 893841

sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_hoi_scoring_esnb trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=8 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871039/epoch=39-step=20840.ckpt'"
# 893846


# analogy
sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vaec_scoring_esnb trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=64 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 893838

sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=vasr_scoring_esnb trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=32
# 893843

sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_hoi_scoring_esnb trainer=test every_n_train_steps=50 every_n_epochs=null batch_size=8 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871037/epoch=49-step=52100.ckpt'"
# 893844

### Run for a day - simple MLP (Linear-ReLU-Linear scoring module)

# BONGARD single

## 10 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb batch_size=64 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'"
# 893857 wtf? # 893941 # 894010
# 894867 (scoring linear output 2)

## 20 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb batch_size=16 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'"
# 893861 wtf? # 893942 # 894011

## real
#./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py ++trainer.limit_train_batches=10 +experiment=bongard_hoi_scoring_esnb batch_size=16
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_esnb batch_size=16
# 893849 wtf? # 893943 # 894012 # 894069 # 894868 # 894878
./scripts/run_bulk_2_0.sh 2 894878 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_esnb batch_size=16
# 894994, 894995


## logo with vit
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb_vit batch_size=16
# 894996

# BONGARD dual

## 10 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_hoi_scoring_esnb batch_size=8 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'"
# 893850 wtf? # 893944 # 893983 # 894013 # 894880
## 20 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_hoi_scoring_esnb batch_size=8 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'"
# 893851 wtf? # 893945 # 893984 # 894014 # 894882

## dual with vit
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_hoi_scoring_esnb_vit batch_size=4
# 894998 # 895004


# ANALOGY single

## 10 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring_esnb batch_size=128 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'"
# 893859 wtf? # 893946 # 893997
## 20 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring_esnb batch_size=64 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'"
# 893860 wtf? # 893947 # 894015

## real
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_esnb batch_size=32
# 893854 works?wtf?
./scripts/run_bulk_2_0.sh 2 893854 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_esnb batch_size=32
# 894020, 894021

## vaec with vit
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring_esnb_vit batch_size=32
# 895003

# ANALOGY dual

## 10 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_vasr_scoring_esnb batch_size=8 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'"
# 893855 wtf? # 893948 # 894022 # 894883
## 20 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_vasr_scoring_esnb batch_size=8 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'"
# 893856 wtf? # 893949 # 894023 # 894884

## dual with vit
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_vasr_scoring_esnb_vit batch_size=8
# 895005

### Embedding without padding (not universal approach)

#
# BONGARD single

## 10 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb batch_size=64 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871045/epoch=158-step=80931.ckpt'" model.relation_module.z_size=64
# 894024

## 20 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb batch_size=16 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871046/epoch=56-step=58026.ckpt'" model.relation_module.z_size=64
# 894027



# ANALOGY single

## 10 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring_esnb batch_size=128 model.encoders.0.num_slots=10 "model.encoders.0.ckpt_path='/app/model_checkpoints/871049/epoch=25-step=13546.ckpt'" model.relation_module.z_size=64
# 894028

## 20 slots
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring_esnb batch_size=64 model.encoders.0.num_slots=20 "model.encoders.0.ckpt_path='/app/model_checkpoints/871052/epoch=12-step=13546.ckpt'" model.relation_module.z_size=64
# 894029

# with encoder unfrozen
# TODO

sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/create_embeddings_datasets.py --data_root=/app/data --out_root=/app/out --model_name=vit_large_patch32_384 --chunk_size=4 --run_logo
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/create_embeddings_datasets.py --data_root=/app/data --out_root=/app/out --model_name=vit_large_patch32_384 --chunk_size=4 --run_hoi
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/create_embeddings_datasets.py --data_root=/app/data --out_root=/app/out --model_name=vit_large_patch32_384 --chunk_size=4 --run_vaec
sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/create_embeddings_datasets.py --data_root=/app/data --out_root=/app/out --model_name=vit_large_patch32_384 --chunk_size=4 --run_vasr

# 895044 (895048 895065 895067) (895046 895066 895068) 895047

sbatch --time=0-00:05:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb_vit-2 batch_size=128
# 895073

# Trying larger scoring/relation module

sbatch --time=1-00:00:00 --mem=40GB --gpus=1 scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb_vit-2_larger batch_size=128

bongard_logo_scoring_esnb_vit-2
bongard_hoi_scoring_esnb-2
vaec_scoring_esnb_vit-2
vasr_scoring_esnb-2

bongard_logo_hoi_scoring_esnb_vit-2
vaec_vasr_scoring_esnb_vit-2

bongard_logo_scoring_esnb_vit-2_larger
bongard_hoi_scoring_esnb-2_larger
vaec_scoring_esnb_vit-2_larger
vasr_scoring_esnb-2_larger

bongard_logo_hoi_scoring_esnb_vit-2_larger
vaec_vasr_scoring_esnb_vit-2_larger



./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb_vit-2 batch_size=128
# 895077
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_esnb-2 batch_size=256
# 895083
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring_esnb_vit-2 batch_size=256
# 895084
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_esnb-2 batch_size=256
# 895085 # 895151
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_hoi_scoring_esnb_vit-2 batch_size=256
# 895081
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_vasr_scoring_esnb_vit-2 batch_size=256
# 895086 # 895152


./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_scoring_esnb_vit-2_larger batch_size=128
# 895095
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_hoi_scoring_esnb-2_larger batch_size=128
# 895096
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_scoring_esnb_vit-2_larger batch_size=128
# 895097 # 895155
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vasr_scoring_esnb-2_larger batch_size=128
# 895098 # 895153 # 895156
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=bongard_logo_hoi_scoring_esnb_vit-2_larger batch_size=128
# 895099
./scripts/run_bulk.sh 1 "--time=1-00:00:00 --mem=40GB --gpus=1" scripts/run.sh src/main.py +experiment=vaec_vasr_scoring_esnb_vit-2_larger batch_size=128
# 895100 # 895154 # 895157
